---
title: Koordlet Support Net QOS
authors:
  - "@lucming"
reviewers:
  - "@eahydra"
  - "@FillZpp"
  - "@hormes"
  - "@jasonliu747"
  - "@stormgbs"
  - "@zwzhang0107"
creation-date: 2023-06-12
last-updated: 2023-06-12
---
# Koordlet Support Net QOS

## Table of Contents

<!--ts-->
- [Koordlet Support Net QOS](#koordlet-support-net-qos)
  - [Table of Contents](#table-of-contents)
  - [Glossary](#glossary)
  - [Summary](#summary)
  - [Motivation](#motivation)
    - [Goals](#goals)
  - [Proposal](#proposal)
    - [User Stories](#user-stories)
      - [Story 1](#story-1)
      - [Story 2](#story-2)
      - [Story 3](#story-3)
      - [Story 4](#story-4)
      - [Story 5](#story-5)
    - [Implementation Details](#implementation-details)
      - [API](#api)
      - [koordlet](#koordlet)
      - [koord-scheduler](#koord-scheduler)
  - [Implementation History](#implementation-history)
<!--te-->

## Glossary
[net_cls cgroup](https://www.kernel.org/doc/Documentation/cgroup-v1/net_cls.txt)  

[tc](https://man7.org/linux/man-pages/man8/tc.8.html) (traffic control)  

[ipset](https://linux.die.net/man/8/ipset)

## Summary

this pr is to enable the koordinator to support `net qos`, support for allocating network bandwidth to pods according to 
pod priority, prioritize the use of network bandwidth by advanced pods, and higher priority pods can steal network bandwidth 
from lower priority pods, Avoid offline tasks using up all the bandwidth and affecting the normal operation of online services.

## Motivation

Koordinator's current implementation of pod qos is mainly based on cpu and memory dimensions, but in fact, network bandwidth, 
as a physical resource on a node, is also a very important part. If all pods on the node can use the network bandwidth resources 
on the node without restriction, many problems may occur, eg: 
important pods are not allocated network bandwidth because of resource hogging, unbalanced distribution of cluster resources, etc.


Therefore, the factor of network bandwidth needs to be considered.


### Goals

- Limits the amount of network bandwidth a process can use. Includes ingress and egress.
- Total net bandwidth capacity of nodes. reported in `koordlet`.
- Report node/pod/container network bandwidth usage information in `koordlet`.
- Provide configurable scheduling plugins to enable pods to be scheduled to nodes with low network bandwidth usage.
- Support eviction based on network bandwidth.

## Proposal

### User Stories

#### Story 1

As a cluster manager, it is hoped that the pods in the cluster can be prioritized to allocate network bandwidth. 
The higher the priority of the pod, the easier it is to allocate network bandwidth. In other words, 
we want the more important pods to use network bandwidth first.

#### Story 2

As an application manager, I want to ensure that my processes have a guaranteed minimum amount of network bandwidth to use when 
the node load is high, and to be able to use as much network bandwidth as possible when the node network bandwidth usage is relatively low.

#### Story 3

As a cluster manager, when scheduling, it is hoped that pods can be scheduled to nodes with relatively idle network bandwidth 
to ensure that new pods can use more network bandwidth, or to avoid affecting process execution performance due to lack of network bandwidth. 
And make the use of node resources in the cluster relatively balanced.

#### Story 4

As a cluster manager, if a node has high network bandwidth usage for a long period of time, we want to be able to evict 
some of the offline pods to run on other idle nodes.

#### Story 5
For some system processes, they should have the highest priority and must not be disturbed by pods at any time, 
so some resources need to be set aside for system processes.


### Implementation Details

Cgroup has a subsystem called `net_cls`, which is used to control the use of network resources by processes. It does not 
directly control network reading and writing, but marks network packets. The specific network packet control is handled by the `tc` mechanism.

You can set the `classid` of a process through the `net_cls` cgroup, so that packets going out from this process will be marked 
with the `classid`, which is the `class` of the qdisc, and then create a cgroup type filter through `tc` to match the specific `class`, 
to achieve network bandwidth limitation based on the `cgroup`.

Because the cgroup here is set to `classid`, `net_cls` is used with `tc` to support only sortable qdiscs. but this only works 
with processes on the host network. For non-host network containers the packets need to be re-tagged with the net `classid`, 
which is done here via `iptables`.

When `koordlet` starts, it creates the `tc` rules and the associated `ipset` objects on the physical `NIC` of the host. 
Each `tc` `class` will have an `ipset`. This `ipset` declares a group of pods. This group of pods has the same `tc` class priority, 
and then share the network bandwidth in this `tc` class. By default, each `tc` `class` can use up all the network bandwidth of the node. 
Afterwards, the `ipset` objects of the three `tc` classes `high_class`/`mid_class`/`low_class` are updated through the create/delet event 
of the watch `pod`. So that the newly created pod can be matched to a `tc` class.

![image](/docs/images/netqos.jpg)  

Logic for `htb qdisc` selection of specific classes:
1. The `htb` algorithm starts at the bottom of the `class` tree and works its way up to find `classes` with the `CAN_SEND` status.
2. If there are more than one `class` in the layer in the `CAN_SEND` state then the `class` with the highest priority (lowest value) 
is selected. After each `class` has sent its own `quantum` bytes, it is the next `class`'s turn to send.

Configuration of parameters for the specific class corresponding to each priority pod:
|  PRIO    | HIGH |  MID  |  LOW  |
| ----     | ---- | ----  | ----  |
| net_prio | 0    | 1     | 2     |
| net_cls  | 1:2  | 1:3   | 1:4   |
| htb.rate | 40%  | 30%   | 30%   |
| htb.ceil | 100% | 100%  | 100%  |

Specific setup method:
```bash
# With an entire network bandwidth of 1000Mbit, the following rules are created.
tc qdisc add dev eth0 root handle 1:0 htb default 1
tc class add dev eth0 parent 1:0 classid 1:1 htb rate 1000Mbit
tc class add dev eth0 parent 1:1 classid 1:2 htb rate 400Mbit ceil 1000Mbit prio 0
tc class add dev eth0 parent 1:1 classid 1:3 htb rate 300Mbit ceil 1000Mbit prio 1
tc class add dev eth0 parent 1:1 classid 1:4 htb rate 300Mbit ceil 1000Mbit prio 2
ipset create high_class hash:net
iptables -t mangle -A POSTROUTING -m set --match-set high_class src  -j CLASSIFY --set-class 1:2
ipset create mid_class hash:net
iptables -t mangle -A POSTROUTING -m set --match-set mid_class src  -j CLASSIFY --set-class 1:3
ipset create low_class hash:net
iptables -t mangle -A POSTROUTING -m set --match-set low_class src  -j CLASSIFY --set-class 1:4
```


#### API
```go
const (
	MAJOR_ID            = 1
	QDISC_MINOR_ID      = 0
	ROOT_CLASS_MINOR_ID = 1
	HIGH_CLASS_MINOR_ID = 2
	MID_CLASS_MINOR_ID  = 3
	LOW_CLASS_MINOR_ID  = 4

	// 0-7, In the round-robin process, classes with the lowest priority field are tried for packets first.
	HIGH_CLASS_PRIO = 0
	MID_CLASS_PRIO  = 1
	LOW_CLASS_PRIO  = 2

	// Maximum rate this class and all its children are guaranteed. Mandatory.
	// attention: the values below only represent the percentage of bandwidth can be used by different tc classes on the host,
	// the real values need to be calculated based on the physical network bandwidth.
	// eg: eth0: speed:200Mbit => high_clss.rate = 200Mbit * 40 / 100 = 80Mbit
	DEFAULT_RATE_PERCENTAGE_HIGH_CLASS = 40
	DEFAULT_RATE_PERCENTAGE_MID_CLASS  = 30
	DEFAULT_RATE_PERCENTAGE_LOW_CLASS  = 30

	// Maximum rate at which a class can send, if its parent has bandwidth to spare.  Defaults to the configured rate,
	// which implies no borrowing
	DEFAULT_CEIL_PERCENTAGE = 100

	DEFAULT_INTERFACE_NAME = "eth0"
)

var (
	rootClass = netlink.MakeHandle(MAJOR_ID, ROOT_CLASS_MINOR_ID)
	highClass = netlink.MakeHandle(MAJOR_ID, HIGH_CLASS_MINOR_ID)
	midClass  = netlink.MakeHandle(MAJOR_ID, MID_CLASS_MINOR_ID)
	lowClass  = netlink.MakeHandle(MAJOR_ID, LOW_CLASS_MINOR_ID)
)

type NetQosManager struct {
	resmanager *resmanager

	// this is the physical NIC on host, default eth0
	interfName string
	interfID   int
	speed      uint64

	// for executing the iptables command.
	iptablesHandler *iptables.IPTables
	// for executing the tc and ipset command.
	netLinkHandler netlink.Handle
}

func NewNetQosManager(resmanager *resmanager) *NetQosManager {
	ifName := getInterfNameByDefaultRoute()
	speedStr, _ := getSpeed(ifName)
	linkid := getlinkid(ifName)
	ipt, _ := iptables.New()

	n := NetQosManager{
		resmanager:      resmanager,
		interfName:      ifName,
		interfID:        linkid,
		speed:           uint64(speedStr),
		iptablesHandler: ipt,
		netLinkHandler:  netlink.Handle{},
	}
	n.Init()

	return &n
}

func (n *NetQosManager) Init() {
	n.EnsureQdisc()
	n.EnsureClasses()
	n.EnsureIpset()
	n.EnsureIptables()
	// ...
}

func (n *NetQosManager) Reconcile() {
	pods := n.resmanager.statesInformer.GetAllPods()
	for _, pod := range pods {
		ipsetentry := netlink.IPSetEntry{
			IP: net.ParseIP(pod.Pod.Status.PodIP),
		}
		netqos := extension.GetPodNetQoSClass(pod.Pod)
		ipsetName := getIpsetNameByNetQoS(netqos)

		if err := netlink.IpsetAdd(ipsetName, &ipsetentry); err != nil {
			klog.Warningf("failed to write ip %s from pod(%s/%s) to ipset(%s), err=%v", pod.Pod.Status.PodIP, pod.Pod.Name, pod.Pod.Namespace, ipsetName, err)
		}
	}
}
```

#### koordlet
1. Netqos  
   The `koordlet` initializes the `tc`, `iptables`, and `ipset` rules when it starts, after which it only needs to watch the pod 
   to update the `ipset` of the `tc` class corresponding to that pod
2. Eviction based on network bandwidth  
   This part is the same idea as the memory based eviction implementation, set a water level line, 
   based on historical network bandwidth data (average) and start evicting offline pods if they exceed the prefabricated. 
   The network bandwidth usage of the node and container is collected and used to determine which offline pods to evict. 
   The final network bandwidth usage information is also synced to the `nodemetric` crd. Roughly as follows:
  ```yaml
apiVersion: slo.koordinator.sh/v1alpha1
kind: NodeMetric
status:
  nodeMetric:
    aggregatedNodeUsages:
      - duration: 5m0s
        usage:
          p50:
            resources:
              cpu: 3756m
              memory: 11688596Ki
              netEgress: '2393'
              netIngress: '713'
          p90:
            ...
          p95:
            ...
          p99:
            ...
      - duration: 10m0s
        ...
      - duration: 30m0s
        ...
    nodeUsage:
      resources:
        cpu: 3774m
        memory: '11956434229'
        netEgress: '40719'
        netIngress: '6201'
  podsMetric:
    - name: node-local-dns-4kwgp
      namespace: kube-system
      podUsage:
        resources:
          cpu: 6m
          memory: '40663129'
          netEgress: '6922'
          netIngress: '40191'
  ```


#### koord-scheduler
A `NetBandwidth` scheduler plugin needs to be added to score the node according to the node network bandwidth load. 
The higher the node network bandwidth load, the lower the score, so as to ensure that the newly created pod can be scheduled 
to a node with relatively idle network bandwidth.
```
score = (node.capacity.netbandwidth - node.netbandwidth.used) * int64(framework.MaxNodeScore)) / node.capacity.netbandwidth
```


## Implementation History

- [X]  06/12/2023: Open PR for initial draft
